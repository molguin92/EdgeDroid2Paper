\section{Estimating execution time distributions at runtime}\label{sec:model}

A key parameter in \cref{eq:tnRayleigh,eq:tnRayleigh:samples,eq:tnRayleigh:energy} that we have yet to discuss is \ensuremath{\sigma}.
This term corresponds to the expected execution time of the current step.
In~\cite{moothedath2021energy,moothedath2022energy1,moothedath2022energy2}, \ensuremath{\sigma} is assumed to be known and constant during a \gls{WCA} task.
We argue, however, that works such as~\cite{olguinmunoz2021impact} show that the assumption of a constant \ensuremath{\sigma} is unrealistic, as users change their timing profile depending on the perceived responsiveness of the system.
Furthermore, users have varying levels of sensitivity to the effects of reduced system responsiveness, and thus may exhibit different \ensuremath{\sigma} for the same system state.

In this section, we therefore propose the development of a stochastic model of human execution times for the estimation and prediction of \ensuremath{\sigma}.
This model is built using data from our previous work in~\cite{olguinmunoz2021impact}, and operates at the same scale as our optimization framework, that is on individual steps in a \gls{WCA} task.
It takes as input variables describing the current responsiveness of the \gls{WCA} application, and outputs a distribution of possible execution times for the next step of the task.
The model can thus be used to estimate real-time, updated values for \ensuremath{\sigma} at the end of a, which in turn can then be used to calculate quasi-optimum aperiodic sampling intervals for the upcoming step.
Additionally, we design the model to be parameterizable with variables describing individual differences between modeled users, allowing for greater variety in emulation.

\subsection{Human reaction to changes in \gls{WCA} application responsiveness}\label{sec:model:insights}

In~\cite{olguinmunoz2021impact}, we studied the effects of system responsiveness on human behavior in step-based \gls{WCA} in a controlled experiment.
We employed a modified and instrumented version of the previously mentioned step-based LEGO \gls{WCA}~\cite{chen2015early}.
\num{40} subjects interacted with the assistant, performing a \num{169}-step task while we altered the responsiveness in realtime and captured key application and task performance metrics.
Additionally, we employed questionnaires to evaluate personality traits of the participants, and correlated these results with the task performance metrics.

Below, we summarize our findings and use them to define as objectives for the modeling of execution times.
Our first finding relates to the immediate effects of system slow-down on human behavior.
We found that system slow-down induces \emph{additional} behavioral slow-down which scales with the decrease in system responsiveness.
Compared to the unimpaired case, participants were on average \SI{12}{\percent} slower when subject to a mean \gls{TTF} of \SI{2.475}{\second}, and \num{26} percentage points slower at a mean \gls{TTF} of \SI{4.5}{\second}.
This leads to a straightforward modeling objective; higher \glspl{TTF} should result in higher execution times.

Next, we found that humans get faster at performing steps as the task progresses at lower \glspl{TTF}.
For sequences of \num{12} steps in an unimpaired application state, humans executed the final four steps on average \SI{36}{\percent} faster than they did the first four.
However, this effect is dampened by reduced system responsiveness, and actually inverts at the highest levels of system impairment;
humans actually become progressively slower the longer they spend in a degraded system state.
We reformulate this finding as a three-pronged modeling objective.
When subject to a series of steps at the same level of impairment, desired behavior depends on the level:
\begin{enumerate*}[itemjoin={{; }}, itemjoin*={{; and }}]
    \item at low \glspl{TTF}, the model should speed up; i.e.\ execution times should decrease
    \item at medium \glspl{TTF}, execution times should remain more or less the same
    \item at high \glspl{TTF}, execution times should increase
\end{enumerate*}.

Our third finding involves the behavior of humans after a slow-down in system responsiveness subsides.
We found that the effects of system slow-down on human behavior remain for a while even after system responsiveness improves.
Specifically, these effects are noticeable for at least \num{4} steps after the return to a high-responsiveness state.
The reformulation of this insight as a modeling objective is once again trivial; after a period of low system responsiveness, execution times should remain higher than otherwise for at least four steps after a return to a higher system responsiveness.

Finally, we found that the effects of system responsiveness on human reaction and behavior are modulated by individual levels of personality characteristics and focus.
In particular, we identified \emph{neuroticism}, a trait from the \gls{BFI}, as a the main modulating factor for differences between users.
We discuss this further in \cref{ssec:moderationeffects}.
As a modeling objective, we express this as a requirement for the model to accept some parameter which modulates the strength of the modeled effects.

\subsection{Generating realistic execution times}\label{ssec:model:exectimes}

To design and build a probabilistic model capable of generating, at runtime, realistic distributions of human execution times for \gls{WCA}, we employ the above insights together with the data collected for~\cite{olguinmunoz2021impact}.
These distributions are updated at each step of the task, based on the current and historical impairment of the \gls{WCA} system, as well as salient individual-difference measures such as \emph{neuroticism}.

As discussed in \cref{sec:model:insights}, the dataset from~\cite{olguinmunoz2021impact} consists of timing and performance metrics as well as personality trait scores for each participant.
We use this data to construct a model for the generation of execution times by, in simplified terms, grouping execution time samples according to discretized levels of neuroticism and a weighted rolling average of \gls{TTF}.
The resulting collections of execution times represent the distributions of these values for users with specific levels of neuroticism, interacting with systems at specific states of impairment and recent histories of impairment.
These distributions can also then be sampled to produce new, realistic execution times.
In the following, we detail the step-by-step processing of this data and construction of the probabilistic model.

In~\cite{olguinmunoz2021impact}, we used \emph{delay} and execution time as the experimental variables for system responsiveness and human behavior, respectively.
\emph{Delay} corresponded to a temporarily fixed time duration for the processing interval of each frame.
That is, if during a series of steps delay was set to \( D \) seconds, the feedback for each frame was provided to the user \( D \) seconds after frame capture.
The correlation between this variable and execution time was then studied.

In order to adapt this data to our system model, we begin by reparameterizing delay into \gls{TTF}.
We do this by noting that \( t_c \), and conversely the wait time \( \mathcal{W} \) of a step, can be assumed to be uniformly distributed in the interval \( [t_{n - 1}, t_n] \) without loss of generality.
Thus, we convert from nomimal delay to \gls{TTF} simply through multiplication by a factor \num{1.5}.

\begin{table*}[]
    \centering
    \caption{%
        Default bins used for model parameter levels.
        Values have been rounded to two decimal places.
    }
    \label{tab:defaultbins}
    \begin{tabular}{@{}lccccccc@{}}
        \toprule
        \textbf{Parameter} & \textbf{Low} & & & \textbf{Medium} & & & \textbf{High}         \\ \midrule
        Neuroticism        & \( [0, 0.5) \) & & & & & & \( [0.5, 1.0] \)      \\
        Weighted TTF & \([0.0], 0.82]\) & \((0.82, 1.53]\) & \((1.53, 2.08]\) & \((2.08, 2.67]\) & \((2.67, 3.45]\) & \((3.45, 4.13]\) & \((4.13, \infty]\)
    \end{tabular}
\end{table*}

Next, the \num{6760} data points are arranged in a table together with identifiers for the subjects, their normalized neuroticism score, and a sequence number for each step.
We calculate rolling weighted averages of the \glspl{TTF} of each of the \num{40} individual repetitions of the data.
We use exponentially decaying weights for the most recent \num{12} steps, defined in \cref{eq:weights}, such that the most recent \gls{TTF} accounts for roughly \SI{50}{\percent} of the rolling average, the second-most recent for \textasciitilde\SI{25}{\percent}, the third-most \textasciitilde\SI{12}{\percent}, and so on.\begin{equation}\label{eq:weights}
    w_{n - i} = 
    \left\{ \begin{array}{ll}
        \frac{e^{-0.7 i}}{\sum\limits^{12}_{j=1} e^{-0.7 j}} & 1 \leq i \leq 12 \\
        & \\
        0 & i > 12
    \end{array} \right.
\end{equation}

We pad the data for each run with \num{12} copies of the first \gls{TTF} in order to ensure sensible values for the first twelve steps; this padding is removed after the weighted values have been calculated.
This weighing ensures that drastic changes in \gls{TTF} are significantly remembered by the model for at least \num{4} steps --- in line with our previous findings on human behavior~\cite{olguinmunoz2021impact} --- and are subsequently quickly forgotten.
\todo[inline]{Do we need more clarification on this?}

The resulting weighted \glspl{TTF} are then binned into continuous ranges by splitting the data on the \num{7}-quantiles.
The number of quantiles was determined experimentally to be a good binning of the total range of \glspl{TTF} as it resulted in a mostly even distribution of number of samples per bin.
The exact resulting bins of this operation on the data are presented in \cref{tab:defaultbins}.

Next, the data is further tagged according to its associated level of normalized neuroticism.
For this work, we use two levels, \emph{low} and \emph{high}, the exact values for which can also be seen in \cref{tab:defaultbins}.
After this preprocessing has been finished, the data is ready to be used for the generation of execution times:

\begin{enumerate}
    \item At the end of each step, the measured \gls{TTF} is fed into the model.
    \item The model calculates a weighted average over the latest \num{12} steps using the weights defined in \cref{eq:weights}.
    \item The resulting weighted \gls{TTF} is binned into the corresponding range (\cref{tab:defaultbins}).
    \item The model then filters the pre-processed data to find execution time samples associated with this same discretized weighted \gls{TTF}.
    \item The selected samples are further filtered to match the desired level of neuroticism.
    \item Finally, the grouped samples are used to either generate an empirical \gls{PDF} or fit a theoretical \gls{PDF} for use in the sampling of the next step in the task.
    % \item The remaining samples are then used to output a realistic execution time, either by
    % \begin{itemize}
    %     \item directly sampling the execution time values;
    %     \item or, using \gls{MLE} to fit a distribution to the execution time samples and then sampling the distribution instead.
    % \end{itemize}
\end{enumerate}

% The distribution chosen for the second variant described above corresponds to the \gls{exGaussian} distribution, previously discussed in \cref{ssec:moderationeffects}.

% We implement these two variants of the model in Python 3.10, and verify their correct behavior in the following.

\subsubsection{Verifying the behavior of the timing model}\label{ssec:model:verification}

\begin{figure}
    \centering
    \includegraphics[width=.9\columnwidth]{figs/new_model/ttf_to_exectime}
    \caption{%
        Effects of feeding three different \glspl{TTF} (\emph{low}, \SI{0}{\second}, \emph{medium}, \SI{2.5}{\second}, or \emph{high}, \SI{5}{\second}) into the model on the generated execution times.
        Higher \glspl{TTF} directly lead to higher execution times.
        Error bars indicate the \SI{95}{\percent} \gls{CI}.
    }\label{fig:ttf_to_exectime}
\end{figure}

\cref{fig:ttf_to_exectime} shows the mean execution time outputted by the model when fed three different levels of \gls{TTF} (\emph{low}, \SI{0}{\second}, \emph{medium}, \SI{2.5}{\second}, or \emph{high}, \SI{5}{\second}).
These results were generated by first warming up the model by feeding it \num{25} \glspl{TTF} selected at random from the data before feeding it the desired input \gls{TTF}, and recording the generated execution time.
This procedure is repeated \num{600} times for each configuration and target \gls{TTF}.
The resulting mean execution times match precisely the desired behavior mentioned in \cref{sec:model:insights}, with the difference in mean execution times at low versus high \glspl{TTF} reaching \SI{14}{\percent} (\textasciitilde\SI{5.6}{\second} to \textasciitilde\SI{6.4}{\second}) in the worst case (high neuroticism configuration).
Additionally, we can observe the effects of neuroticism on generated execution times.
At low neuroticism, the average difference between execution times at low versus high \glspl{TTF} was of roughly \SI{6.2}{\percent}, compared to \SI{12.5}{\percent} at high neuroticism.

\begin{figure}
    \centering
    % \includegraphics[height=25em]{figs/new_model/exectime_over_steps}
    \includegraphics[width=.8\columnwidth]{figs/new_model/exectime_over_steps}
    \caption{%
    Effects of prolonged exposure to constant levels of system impairment on the model.
    At low (\SI{0}{\second}) \gls{TTF}, the models speed-up over time; conversely at high (\SI{5.0}{\second}) \gls{TTF}, the models either present no change or drastically increase their generated execution times, depending on the level of neuroticism.
    Error bands indicate \SI{95}{\percent} \gls{CI}.
    }\label{fig:exectimeduration}
\end{figure}

Next, \cref{fig:exectimeduration} shows the evolution of generated execution times while the model is subject to a fixed \gls{TTF}, either \emph{low} (\SI{0}{\second}) or \emph{high} (\SI{5}{\second}).
These results were generated by first warming up the model with \num{25} random \glspl{TTF}, and then recording the generated execution times over a sequence of \num{12} steps at a fixed \gls{TTF}; this procedure is repeated \num{600} times for each configuration and target \gls{TTF}.
Once again, we see here behavior matching what we expect from \cref{sec:model:insights}.
At low \glspl{TTF}, the model is on average, across all configurations, \SI{8.2}{\percent} faster at step \num{12} when compared to step \num{1}.
At high \glspl{TTF}, the behavior changes depending on the level of neuroticism of the model.
Low neuroticism models basically do not change their execution times, whereas high neuroticism configurations are on average \SI{10}{\percent} slower after \num{12} steps.
This is once again in line with our previous findings, as we had previously concluded that humans tend to speed up during a task, but that this speed-up is hindered and eventually reversed as system responsiveness decreases, and that the strength of this effect is correlated with neuroticism.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/new_model/transitions}
    \caption{%
        Effects of changes in system impairment on subsequently generated execution times.
        These effects linger on after the change, and thus execution times immediately after a transition are either consistently lower or higher than otherwise at the new \gls{TTF}, depending on the old \gls{TTF}.
        Error bars indicate \SI{95}{\percent} \gls{CI}.
    }\label{fig:transitions}
\end{figure*}

Finally, in \cref{fig:transitions} we showcase the behavior of the model when comparing execution times generated immediately after a change in system responsiveness.
We generate these results by first warming up the model by feeding it a fixed \gls{TTF} (which we will refer to as the \emph{origin} \gls{TTF}) \num{25} times.
Next, another \gls{TTF} value (the \emph{destination} \gls{TTF}) is fed to the model, and we record the output execution time.
Each sample is tagged according to the relation between origin and destination \gls{TTF}, either lower to higher, higher to lower, or equal.
As before, we run \num{600} repetitions of this procedure for each combination of model configuration, origin \gls{TTF}, and destination \gls{TTF}.
Execution times generated immediately after a transition from a higher \gls{TTF} into a lower one are consistently higher than execution times generated without a preceding change in \glspl{TTF}.
Conversely, execution times are consistently lower than otherwise immediately after a change from a lower \gls{TTF} into a higher one.
These results are once again in line with our findings in~\cite{olguinmunoz2021impact}, in which we found lingering effects of transitions between levels of system impairment on human execution times.

% \subsection{Generating realistic samples}\label{ssec:model:frames}

% Apart from the aforementioned timing and performance data, for~\cite{olguinmunoz2021impact} we also recorded all collected video frame samples together with matching metadata.
% For each video frame submitted to the \gls{WCA} during the tasks, we recorded
% \begin{inlineenum}
%     \item the raw video frame captured
%     \item sample submission timestamp
%     \item \gls{WCA} processing completed timestamp
%     \item result or acknowledgement returned timestamp
%     \item a tag representing the result of the \gls{WCA} processing
% \end{inlineenum}.
% The tags assigned corresponded to:
% \begin{description}[font={\bfseries\ttfamily}, wide]
%     \item[SUCCESS:] frames which triggered a transition to a new step (or the correction of a previous mistake) in the logical task model of the \gls{WCA}, and thus cause the generation of feedback to the user.
%     \item[REPEAT:] frames which captured the same board state as the previous successful frame, and thus produced no feedback.
%     \item[LOW\_CONFIDENCE] frames for which the image recognition algorithm in the \gls{WCA} did not reach the necessary confidence threshold to interpret it as a valid board state.
%         These frames also produce no feedback.
%     \item[BLANK] frames in which not enough of the board is visible due to noise, movement, occlusion, etc.
%         These frames produce no feedback either.
%     \item[TASK\_ERROR] frames which contained an incorrect board state and thus triggered a transition to a procedural corrective step and the generation of feedback to the user.
%         However, it must be noted that none of the \num{40} participants made any mistakes during the task, and thus no such frames were encountered in the data.
% \end{description}

% We correlate this frame data with the step timing data described in \cref{ssec:model:exectimes} to match frames with their corresponding step execution times.
% We assign to each frame a normalized instant value \( t_\text{norm} \) corresponding to its capture instant \( \tau \) (expressed in seconds since the start of the step) divided by the total execution time \( t_\text{exec} \) of the step:

% \begin{align}
%     \left( t_\text{norm} = \frac{\tau}{t_\text{exec}} \right) \in [0, 1]\label{eq:tnorm}
% \end{align}

% % To exemplify, for a step with execution time \( t_\text{exec} = \SI{10}{\second} \), a frame captured at time \( \tau = \SI{3}{\second} \) from the start of the step, will have a normalized instant value \( t_\text{norm} \):
% % \begin{align}
% %     t_\text{norm} = \frac{\tau}{t_\text{exec}} = \frac{\SI{3}{\second}}{\SI{10}{\second}} = 0.3
% % \end{align}

% This allows us to analyze the distribution of frame tag probabilities as a step progresses, independently of execution times.
% This is illustrated in \cref{fig:frameprobs}.
% Intuitively, \texttt{REPEAT} frames dominate the early instants after a step transition, as the user has not had time to start performing the new instruction and thus the \gls{WCA} keeps capturing frames representing the previous state of the board.
% As the user starts moving and performing actions, \texttt{BLANK} frames start to dominate, as this activity prevents the \gls{WCA} from capturing ``clean'' frames.
% Finally, it must be noted that \texttt{SUCCESS} frames are not included in this probability density plot, as, by definition:
% \begin{equation}
%     P(\text{\texttt{SUCCESS}} | t_\text{norm}) =
%     \left\{ \begin{array}{ll}
%         0 & t_\text{norm} < 1.0    \\
%         1 & t_\text{norm} \geq 1.0
%     \end{array} \right.
% \end{equation}

% That is, any frame captured immediately at or after the execution time has been reached will contain the finished board state and thus correspond to a \texttt{SUCCESS} frame.

% \begin{figure}
%     \centering
%     \includegraphics[width=.9\textwidth]{model_data/frame_probabilities}
%     \caption{%
%         Probability density of frame result tags as a step progresses.
%         Note that \texttt{SUCCESS} frames are not included as --- by definition --- the probability for success frames is \num{1} for all normalized instant values greater than or equal to \num{1.0}.
%     }\label{fig:frameprobs}
% \end{figure}

% Using the above insights, together with the corresponding recorded video frames, we devise a scheme for the procedural generation of a synthetic trace for any step in \gls{WCA} task in the same category as those used in~\cite{olguinmunoz2021impact}.
% We first prepare a discretized representation of the probability density map in \cref{fig:frameprobs}.
% We segment the normalized instant value into a number of discrete bins (\num{25} in this work), and calculate the relative fraction of frames for each category in each bin.
% For each step, given
% \begin{inlineenum}
%     \item a collection of random non-\texttt{SUCCESS}, non-\texttt{REPEAT} frames (at least one frame for each of the \texttt{BLANK} and \texttt{LOW\_CONFIDENCE} categories)
%     \item an appropriate \texttt{SUCCESS} video frame containing the correct state for the step
%     \item an appropriate \texttt{REPEAT} video frame containing the correct state for the \emph{previous} step
% \end{inlineenum},
% we can then procedurally generate a trace by randomly selecting appropriate frames according to the distributions presented in \cref{fig:frameprobs}.
% In other words, for each sampling instant in a step with a given execution time \( t_\text{exec} \):
% \begin{enumerate}
%     \item We calculate \( t_\text{norm} \) according to \cref{eq:tnorm}.
%     \item If \( t_\text{norm} \ge 1.0 \), we select the \texttt{SUCCESS} frame and stop sampling.
%     \item If instead \( t_\text{norm} < 1.0 \), we find the appropriate bin for \( t_\text{norm} \) and then select a frame by performing a weighted random sampling of the frame categories in the normalized instant bin.
% \end{enumerate}

\subsection{Obtaining the model}\label{ssec:model:obtaining}

\todo[inline]{Move to the end or introduction. Expand to say we provide implementation of framework too.}

We provide the model implementations in Python~\num{3.10} as well as the base data to the community as \gls{FOSS}.
All of these are published on the \href{https://github.com/KTH-EXPECA/EdgeDroid2}{\texttt{KTH-EXPECA/EdgeDroid2}} repository on GitHub under a permissive Apache version 2 license.
